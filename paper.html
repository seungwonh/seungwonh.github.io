 <li><i>Recommended Reading:</i> <a href="https://aclanthology.org/Q18-1041/">Data Statements for NLP</a> (Bender and Friedman 2018)</li>
  <li><i>Recommended Reading:</i> <a href="https://aclanthology.org/P18-1128/">The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing</a> (Dror et al. 2018)</li>
  <li><i>Recommended Reading:</i> <a href="https://arxiv.org/abs/2004.03705">Deep Learning Based Text Classification: A Comprehensive Review</a> (Minaee et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D17-1082/">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a> (Lai et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D13-1170/">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a> (Socher et al. 2013)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/P04-1035/">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts</a> (Pang and Lee 2004)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1509.01626">Character-level Convolutional Networks for Text Classification</a> (Zhang et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf">An Empirical Study of Smoothing Techniques for Language Modeling</a> (Goodman 1998)</li>
  <li><i>Software:</i> <a href="https://github.com/kpu/kenlm">kenlm</a></li>
  <li><i>Reference:</i> <a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2337&context=compsci">Maximum entropy (log-linear) language models</a>. (Rosenfeld 1996)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1608.05859.pdf">Using the Output Embedding</a>. (Press and Wolf 2016)</li>
  <li><i>Reference:</i> <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>. (Bengio et al. 2003, JMLR)</li>
    <li><i>Recommended Reading:</i> <a href="https://arxiv.org/pdf/1703.01619.pdf">Neural Machine Translation and Sequence-to-Sequence Models</a> Chapter 7</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D13/D13-1176.pdf">Recurrent Neural Translation Models</a> (Kalchbrenner and Blunsom 2013)</li>
  <li><i>Reference:</i> <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">LSTM Encoder-Decoders</a> (Sutskever et al. 2014)</li>
  <li><i>Reference:</i> <a href="https://ufal.mff.cuni.cz/pbml/108/art-bahar-alkhouli-peter-brix-ney.pdf">Parameter Averaging</a> (Bahar et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1139.pdf">Knowledge Distillation</a> (Kim et al. 2016)</li>
  <li><i>Link:</i> <a href="http://statmt.org/wmt21/">WMT Shared Tasks</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2001.09977">Meena Chatbot</a> (Adiwardana et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">Generation from Images</a> (Karpathy and Li 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D15/D15-1199.pdf">Generation from Structured Data</a> (Wen et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D17/D17-1239.pdf">Challenges in Data-to-document Generation</a> (Wisemen et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/N16-1005/">Generation from Input+Tags</a> (Sennrich et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/N/N16/N16-1149.pdf">Generation from TED Talk Metadata</a> (Hoang et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://www.statmt.org/wmt20/">WMT Transalation Tasks</a></li>
  <li><i>Reference:</i> <a href="https://genie.apps.allenai.org/">GENIE Leaderboard</a></li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P02/P02-1040.pdf">BLEU</a> (Papineni et al. 2002)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1904.09675">BertScore</a> (Zhang et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.704.pdf">BLEURT</a> (Sellam et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.213.pdf">COMET</a> (Rei et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.8.pdf">PRISM</a> (Thompson and Post 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2106.11520">BARTScore</a> (Yuan et al. 2021)</li>
  <li><i>Reference:</i> <a href="http://www.statmt.org/wmt20/pdf/2020.wmt-1.77.pdf">WMT Metrics Shared Task</a> (Mathur et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.751.pdf">Re-evaluating Evaluation in Text Summarization</a> (Bhandari et al. 2020)</li>
    <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a> (Vaswani et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P18-1166.pdf">Slow Transformer Decoding</a> (Zhang et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P18-1008.pdf">Transformer+RNN Hybrid Models</a> (Chen et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1910.05895">Training Transformers on Small Data</a> (Nguyen and Salazar 2019)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/N16-1102.pdf">Structural Biases in Attention</a> (Cohn et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D16-1096.pdf">Coverage Embedding Models</a> (Mi et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1011.pdf">Interpretability w/ Hard Attention</a> (Lei et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1249.pdf">Supervised Attention</a> (Mi et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1706.03872.pdf">Attention vs. Alignment</a> (Koehn and Knowles 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1902.10186">Attention is not Explanation</a> (Jain and Wallace 2019)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.432.pdf">Learning to Deceive w/ Attention</a> (Pruthi et al. 2020)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1138.pdf">Monotonic Attention</a> (Yu et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1602.03001.pdf">Convolutional Attention</a> (Allamanis et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://www.sciencedirect.com/science/article/pii/S0925231218300225">Fine-grained Attention</a> (Choi et al. 2018)</li>
    <li><i>Reference:</i> <a href="http://www.aclweb.org/anthology/K16-1006">context2vec</a> (Melamud et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1802.05365">ELMo: Deep Contextualized Word Representations</a> (Peters et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D19-1410/">Sentence BERT</a> (Reimers and Gurevych 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Bidirectional Transformers</a> (Devlin et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa: Robustly Optimized BERT</a> (Liu et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1906.08237.pdf">XLNet: Autoregressive Training w/ Permutation Objectives</a> (Yang et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://openreview.net/forum?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders as Discriminators</a> (Clark et al. 2020)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D17/D17-1071.pdf">Inference -> Generalization</a> (Conneau et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1711.05732">Paraphrase ID -> Generalization</a> (Wieting and Gimple 2018)</li>
  <li><i>Reference:</i> <a href="http://www.aclweb.org/anthology/W18-5448">Comparison of Training Objectives</a> (Zhang and Bowman 2018)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D18-1103/">Multilingual NMT</a> (Neubig and Hu 2018) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D19-1077/">Multilingual BERT</a> (Wu and Dredze 2019) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/P17-2061/">Domain Tag</a> (Chu et al. 2017) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/Q17-1024/">Language Tag</a> (Johnson et al. 2017) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2020.acl-main.747/">Cross-lingual Model Capacity</a> (Conneau et al. 2020) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/N16-1101/">Separate Encoders and Decoders</a> (Firat et al. 2016) </li>
  <li><i>Reference:</i> <a href="http://proceedings.mlr.press/v97/houlsby19a.html">Adapters</a> (Houlsby et al. 2019) </li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2109.07437">Optimized Task Weighting</a> (Dery et al. 2021) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/P19-1301/">Choosing Transfer Languages</a> (Lin et al. 2019) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2020.emnlp-main.635/">Transferability Across Tasks</a> (Vu et al. 2020) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D19-1432/">Distributionally Robust Language Modeling</a> (Oren et al. 2019) </li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2109.04020">Distributionally Robust Machine Translation</a> (Zhou et al. 2021) </li>
  <li><i>Reference:</i> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4670">Dealing w/ Script Differences</a> (Rijhwani et al. 2019) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/H01-1035/">Annotation Projection</a> (Yarowsky et al. 2001) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/N16-1161/">Multilingual Language Modeling</a> (Tsvetkov et al. 2016) </li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D19-1103/">Multilingual Structured Prediction</a> (Meng et al. 2019) </li>
    <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/D/D13/D13-1020.pdf">MCTest</a> (Richardson et al. 2013)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D17/D17-1083.pdf">RACE</a> (Lai et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/D/D16/D16-1264.pdf">SQuAD</a> (Rajpurkar et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1147.pdf">TriviaQA</a> (Joshi et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P16-1223">A Thorough Examination of the CNN/Daily Mail Task</a> (Chen et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D17/D17-1214.pdf">Adversarial Examples in SQuAD</a> (Jia and Liang 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1808.05326.pdf">Adversarial Construction of Datasets</a> (Zellers et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b8c26e4347adc3453c15d96a09e6f7f102293f71.pdf">Natural Questions</a> (Kwiatkowski et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1506.03340.pdf">Teaching Machines to Read and Comprehend</a> (Hermann et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P/P16/P16-1086.pdf">Attention Sum</a> (Kadlec et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1055.pdf">Attention over Attention</a> (Cui et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1611.01603.pdf">Bidirectional Attention Flow</a> (Seo et al. 2017)</li>
  <li><i>Reference:</i> <a href="">UnifiedQA</a> (Khashabi et al. 2000)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/Q18-1023/">NarrativeQA</a> (Kočiský et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1809.09600.pdf">HotpotQA</a> (Yang et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1904.12106">Dataset Bias in Multi-hop Reasoning</a> (Chen and Durrett 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1410.3916.pdf">Memory Networks</a> (Weston et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1503.08895.pdf">End-to-end Memory Networks</a> (Sukhbaatar et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://proceedings.mlr.press/v48/kumar16.pdf">Dynamic Memory Networks</a> (Kumar et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1609.05284.pdf">Learning to Stop Reading</a> (Shen et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1020.pdf">Coarse-to-fine Question Answering</a> (Choi et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1171.pdf">Reading Wikipedia to Answer Open-Domain Questions</a> (Chen et al. 2017)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2001.11770">BREAK Dataset</a> (Wolfson et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1906.02916">Question Decomposition</a> (Min et al. 2019)</a>
  <li><i>Reference:</i> <a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">Retrieval-augmented Generation</a> (Lewis et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2009.12756">Multi-hop Dense Retrieval</a> (Xiong et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1909.01066">Language Models as Knowledge Bases</a> (Petroni et al. 2019)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2002.08909.pdf">Retrieval-based Language Model Pre-training</a> (Guu et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P19-1329.pdf">Exploring Numeracy in Embeddings</a> (Naik et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/D19-1534.pdf">Do NLP Models Know Numbers</a> (Wallace et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1903.00161">DROP Dataset</a> (Dua et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1912.04971.pdf">Neural Module Networsk for Reasoning Over Text</a> (Gupta et al. 2020)</li>
  <li><i>Reference:</i> <a href="http://papers.nips.cc/paper/6969-end-to-end-differentiable-proving.pdf">End-to-end Differentiable Proving</a> (Rocktäschel and Riedel 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1808.07036">Question Answering in Context</a> (Choi et al. 2018)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1808.07042.pdf">Contextual Question Answering</a> (Reddy et al. 2018)</a>
  <li><i>Highly Recommended Reading:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.485/">Language (Technology) is Power: A Critical Survey of “Bias" in NLP</a> (Blodgett et al. 2020)</li>
<li><i>Reference:</i> <a href="https://aclanthology.org/2021.acl-long.149/">Survey of Race and Racism in NLP</a></li>
<li><i>Reference:</i> <a href="https://dl.acm.org/doi/pdf/10.1145/301353.301410">Ethics of Persuasive Technology</a> (Berdichevsky and Neuenschwander, 1999)</li>
<li><i>Reference:</i> <a href="https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_9.pdf">Measuring and Mitigating Unintended Bias in Text Classification</a> (Dixon et al. 2017)</li>
<li><i>Reference:</i> <a href="http://modeltheory.org/papers/2016counterfactuals.pdf">Counterfactual Thought</a> (Byrne 2016)</li>
<li><i>Reference:</i> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/phpr.12691">Norms in Counterfactual Selection</a> (Fazelpour 2020)</li>
<li><i>Reference:</i> <a href="https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a> (Bolukbasi et al. 2016)</li>
<li><i>Reference:</i> <a href="https://arxiv.org/abs/1608.07187">Semantics derived automatically from language corpora contain human-like biases</a> (Caliskan et al. 2017)</li>
<li><i>Reference:</i> <a href="https://arxiv.org/abs/1903.10561">On Measuring Social Biases in Sentence Encoders</a> (May et al. 2019)</li>
<li><i>Reference:</i> <a href="https://www.pnas.org/content/117/14/7684">Racial disparities in automated speech recognition</a> (Koenecke et al. 2020)</li>
<li><i>Reference:</i> <a href="https://aclanthology.org/2020.acl-main.560/">State and Fate of Linguistic Diversity</a> (Joshi et al. 2020)</li>
<li><i>Reference:</i> <a href="https://arxiv.org/abs/2110.06733">Systematic Disparities in Language Technology Performance</a> (Blasi et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/D/D15/D15-1174.pdf">Joint Entity and Relation Embedding</a> (Toutanova et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1040.pdf">Distant Supervision for Neural Models</a> (Luo et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://papers.nips.cc/paper/3863-modelling-relational-data-using-bayesian-clustered-tensor-factorization.pdf">Relation Extraction w/ Tensor Decomposition</a> (Sutskever et al. 2009)</li>
  <li><i>Reference:</i> <a href="http://www.cs.cmu.edu/~wcohen/postscript/ecml-2010-ni.pdf">Relation Extraction via. KG Paths</a> (Lao and Cohen 2010)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D15/D15-1038.pdf">Relation Extraction by Traversing Knowledge Graphs</a> (Guu et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1702.08367.pdf">Relation Extraction via Differentiable Logic Rules</a> (Yang et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://www.aclweb.org/anthology/P14-2089">Improving Embeddings w/ Semantic Knowledge</a> (Yu et al. 2014)</li>
  <li><i>Reference:</i> <a href="">Improving Embeddings w/ Semantic Knowledge</a> (Yu et al. 2014)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/N/N15/N15-1184.pdf">Retrofitting Word Vectors to Semantic Lexicons</a> (Faruqui et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://www.aclweb.org/anthology/N15-1070">Multi-sense Embedding with Semantic Lexicons</a> (Jauhar et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://www.anthology.aclweb.org/N/N16/N16-1018.pdf">Antonymy and Synonym Constraints for Word Embedding</a> (Mrksic et al. 2016)</li>
  <li><i>Reference:</i> <a href=“https://www.aclweb.org/anthology/D19-1250.pdf”>Language Models as Knowledge Bases?</a> (Petroni et al. 2019)</li>
<li><i>Reference:</i> <a href=“https://www.aclweb.org/anthology/2020.tacl-1.28.pdf”>How Can We Know What Language Models Know?</a> (Jiang et al. 2019)</li>
<li><i>Reference:</i> <a href=“https://www.aclweb.org/anthology/2020.emnlp-main.346.pdf”>AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a> (Shin et al. 2020)</li>
<li><i>Reference:</i> <a href=“https://arxiv.org/pdf/2103.10385.pdf”>GPT Understands, Too</a> (Liu et al. 2021)</li>
<li><i>Reference:</i> <a href=“https://www.aclweb.org/anthology/2020.emnlp-main.437.pdf”>How Much Knowledge Can You Pack Into the Parameters of a Language Model?</a> (Roberts et al. 2020)</li>
<li><i>Reference:</i> <a href=“https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf”>X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models</a> (Jiang et al. 2020)</li>
<li><i>Reference:</i> <a href=“https://arxiv.org/pdf/2002.08909.pdf”>REALM: Retrieval-Augmented Language Model Pre-Training</a> (Guu et al. 2020)</li>
<li><i>Reference:</i> <a href=“https://arxiv.org/pdf/2005.11401.pdf”>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> (Lewis et al. 2020)</li>
